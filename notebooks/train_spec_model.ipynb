{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import timm\n",
    "import yaml\n",
    "import gc\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../hms_pipeline')\n",
    "\n",
    "from hms_pipeline.trainer import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_PATH = \"../configs/hms-configs.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = dict(\n",
    "    trainer=dict(\n",
    "        max_epochs=12,\n",
    "        min_epochs=5,\n",
    "        enable_progress_bar=True,\n",
    "        devices=1,\n",
    "        deterministic=False,\n",
    "        precision=\"16\",\n",
    "    ),\n",
    "\n",
    "    seed =42,\n",
    "    \n",
    "    patience=3,\n",
    "    train_bs=8,\n",
    "    valid_bs=8,\n",
    "    workers=8,\n",
    "    wandb_project=\"hms_v6_2\",\n",
    "    output_dir=\"../models-hmsv6-2/hmsv6-convnexts-3imgs-w020-kl1-lrf\",\n",
    "    fold_dir =  \"/home/maxc/workspace/kaggle-hms/folds\",\n",
    "    train_folds=[0],\n",
    "    # train_folds = [1],\n",
    "    # train_folds = [1, 2, 3, 4],\n",
    "    # train_folds = [0, 1],\n",
    "    # train_folds = [2, 3, 4],\n",
    "    # train_folds = [0, 1, 2, 3, 4],\n",
    "    # train_folds = [2],\n",
    "\n",
    "    ##############################\n",
    "    # Dataset parameters\n",
    "    ##############################\n",
    "    # train_dataset_group=\"eeg_id\",\n",
    "    train_dataset_group=\"unique_label\",\n",
    "    val_dataset_group=\"eeg_id\",\n",
    "\n",
    "    raw_eeg_dir = \"/home/maxc/workspace/kaggle-hms/data/v6/raw_eegs_sosbandclip_2500\",\n",
    "    eeg_spec_dir = \"/home/maxc/workspace/kaggle-hms/data/v6/eeg_specs_h100w250gf5fft1024wl200lc05_norm_const\",\n",
    "    long_spec_dir= \"/home/maxc/workspace/kaggle-hms/data/v6/long_specs\",\n",
    "    \n",
    "\n",
    "    l7_weight = 0.2, # extra inverse weight for votes less than 7\n",
    "\n",
    "\n",
    "    inverse_kl_weight=True,\n",
    "    # inverse_kl_weight=False,\n",
    "    # inverse_kl_mean = [ 0.174031, 0.112700,0.090854, 0.071484,0.136408, 0.414523],\n",
    "    inverse_kl_mean = [1/6, 1/6, 1/6, 1/6, 1/6, 1/6],\n",
    "    kl_multiplier = 1.0,\n",
    "    vote_weight = False,\n",
    "\n",
    "\n",
    "    # Options to control the presence of each image type\n",
    "    # img_types=[\"raw_eeg\", \"eeg_spec\", \"long_spec\"],    \n",
    "    # img_types=[\"full_raw_eeg\", \"raw_eeg\", \"long_spec\"],\n",
    "    # img_types = [\"long_spec\", \"raw_eeg\"]\n",
    "    img_types = [\"long_spec\", \"eeg_spec\", \"full_raw_eeg\", \"raw_eeg\"],\n",
    "\n",
    "\n",
    "    # The order of each area in the image\n",
    "    signals=[\"LL\", \"RL\", \"LP\", \"RP\"],\n",
    "    lrflip_signals=[\"RL\", \"LL\", \"RP\", \"LP\"],\n",
    "\n",
    "    # # new order\n",
    "    # signals = [\"LL\", \"LP\", \"RL\", \"RP\"],\n",
    "    # lrflip_signals = [\"RL\", \"RP\", \"LL\", \"LP\"],\n",
    "\n",
    "    # # old order\n",
    "    # signals = [\"LL\", \"LP\", \"RP\", \"RL\"],\n",
    "    # lrflip_signals = [\"RL\", \"RP\", \"LP\", \"LL\"],\n",
    "\n",
    "  \n",
    "    img_size=[512, 512],\n",
    "\n",
    "    long_spec_ratio = 1.0, # The percentage of long spectrogram of 50s in the image\n",
    "    raw_full_gap = 16, # gap between full raw eeg and raw eeg\n",
    "    center_timespan = 10, # timespan of center on eeg spectrogram\n",
    "\n",
    "    sub_img_size={\n",
    "        \"eeg_spec\": [288, 200],\n",
    "        \"long_spec\": [288, 300],\n",
    "        \"raw_eeg\": [128, 500],\n",
    "        \"full_raw_eeg\": [64, 500],\n",
    "    },\n",
    "\n",
    "\n",
    "    # img_size = [448, 448],\n",
    "    # sub_img_size={\n",
    "    #         \"eeg_spec\": [256, 224],\n",
    "    #         \"long_spec\": [256, 224],\n",
    "    #         \"raw_eeg\": [128, 448],\n",
    "    #         \"full_raw_eeg\": [64, 448],\n",
    "    # },\n",
    "   \n",
    "    # Options to control the vertical flip of sub images\n",
    "    sub_img_vflips={\n",
    "        \"eeg_spec\": [False, False, False, False],\n",
    "        # \"eeg_spec\": [True, False, True, False],\n",
    "        \"long_spec\": [False, False, False, False],\n",
    "        # \"long_spec\": [False, True, False, False],\n",
    "        \"raw_eeg\": [False, False, False, False],\n",
    "        \"raw_eeg\": [False, False, True, True],\n",
    "    },\n",
    "\n",
    "    train_aug_probs={\n",
    "        \"lrflip_prob\": 0.5, # left right flip\n",
    "        \"fbflip_prob\": 0.5,  # front back flip\n",
    "        \"mask_prob\": 0.5, # probablity of masking\n",
    "        \"keep_center_ratio\": 0.2, # ratio of keeping center on eeg spectrogram\n",
    "        \"hflip_prob\": 0.5, # probablity of horizontal flip on spectrogram\n",
    "        \"blur_prob\":0.0, # probablity of blurring spectrogram\n",
    "        \"roll_prob\": 0.5, # probablity of rolling raw eeg\n",
    "        \"neg_eeg_prob\": 0.5, # probablity of negative raw eeg\n",
    "        \"contrast_prob\": 0.5, # probablity of contrast adjustment on raw eeg\n",
    "        \"fuse_prob\": 0.0, # probablity of fusing spectrogram\n",
    "        \"block_prob\": 0.5, # probablity of blocking raw eeg channels\n",
    "        \"noise_prob\": 0.0,  # probablity of adding noise to entire image\n",
    "        \"mask_iter\": 3, # iteration of masking\n",
    "        \"mask_size_ratio\": 0.2, # h/w ratio of each mask size\n",
    "        \"num_block_ch\": 4, # number of blocked channels\n",
    "        \"dummy_votes_prob\": 0.0, # probablity of adding dummy votes\n",
    "        \"num_dummy_votes\": 1, # number of dummy votes\n",
    "    },  # sofar best aug\n",
    "\n",
    "    val_aug_probs={\n",
    "        \"lrflip_prob\": 0.0,\n",
    "        \"fbflip_prob\": 0.0,\n",
    "        \"mask_prob\": 0.0,\n",
    "        \"keep_center_ratio\": 0.0,\n",
    "        \"hflip_prob\": 0.0,\n",
    "        \"blur_prob\": 0.0,\n",
    "        \"roll_prob\": 0.0,\n",
    "        \"neg_eeg_prob\": 0.0,\n",
    "        \"contrast_prob\": 0.0,\n",
    "        \"fuse_prob\": 0.0,\n",
    "        \"block_prob\": 0.0,\n",
    "        \"noise_prob\": 0.0,\n",
    "        \"mask_iter\": 5,\n",
    "        \"mask_size_ratio\": 0.1,\n",
    "        \"num_block_ch\": 4,\n",
    "        \"dummy_votes_prob\": 0.0,\n",
    "        \"num_dummy_votes\": 1,\n",
    "    },\n",
    "\n",
    "    ##############################\n",
    "    # Model parameters\n",
    "    ##############################\n",
    "    # model_type = \"SpecVitModel\",\n",
    "    model_type=\"SpecModel\",\n",
    "\n",
    "    dropout=0.5,\n",
    "    # global_pool = [\"max\", \"avg\"],\n",
    "    global_pool=[\"avg\"],\n",
    "    # global_pool=[\"gem\"],\n",
    "    hidden_size = 8,\n",
    "\n",
    "    # spec_backbone = \"efficientnet_b0\",\n",
    "    # spec_backbone=  \"efficientnet_b2\",\n",
    "    # spec_backbone = \"maxxvitv2_nano_rw_256\",\n",
    "    # spec_backbone=\"convnext_tiny.fb_in22k\",\n",
    "    # spec_backbone = \"convnextv2_tiny.fcmae_ft_in22k_in1k\",\n",
    "    # spec_backbone = \"convnext_base.fb_in22k_ft_in1k\",\n",
    "    # spec_backbone=\"tf_efficientnetv2_m.in21k_ft_in1k\",\n",
    "    spec_backbone= \"convnext_small.fb_in22k\",\n",
    "    # spec_backbone = \"maxvit_tiny_tf_512\",\n",
    "    # spec_backbone = \"maxvit_small_tf_512\",\n",
    "    # spec_backbone = \"maxxvitv2_nano_rw_256.sw_in1k\",\n",
    "    # spec_backbone = \"maxvit_small_tf_512\",\n",
    "    # spec_backbone = \"tf_efficientnetv2_s.in21k\",\n",
    "    # spec_backbone = \"tf_efficientnetv2_m.in21k\",\n",
    "    # spec_backbone = \"swinv2_tiny_window8_256.ms_in1k\",\n",
    "    \n",
    "    ## SpecVitModel Params\n",
    "    vit_model=\"vit_small_patch16_224\",\n",
    "    # vit_model = \"vit_base_patch32_clip_448.laion2b_ft_in12k_in1k\",\n",
    "    # vit_model = \"vit_base_patch16_224.augreg2_in21k_ft_in1k\",\n",
    "    # vit_model=\"swinv2_small_window16_256.ms_in1k\",\n",
    "    # vit_model= \"swinv2_tiny_window16_256.ms_in1k\",\n",
    "    # vit_model = \"swinv2_tiny_window8_256.ms_in1k\",\n",
    "    # vit_model = \"maxvit_tiny_tf_224.in1k\",\n",
    "    # vit_model = \"maxvit_tiny_tf_224\",\n",
    "    # vit_model = \"efficientnet_b0\",\n",
    "    feature_layer=[-1],\n",
    "    # global_pool = \"avg\",\n",
    "    # global_pool = \"max\",\n",
    "    # global_pool = \"avgmax\",\n",
    "    optimizer_params=dict(\n",
    "        lr = 1e-4,\n",
    "        # weight_decay = 0,\n",
    "    ),\n",
    "    use_ema = True,\n",
    "    ema_decay = 0.999,\n",
    "    scheduler=dict(\n",
    "        name=\"CosineAnnealingLR\",\n",
    "        params=dict(\n",
    "            CosineAnnealingLR=dict(T_max=10, eta_min=1e-6),\n",
    "        ),\n",
    "    ),\n",
    ")\n",
    "\n",
    "with open(CONFIG_PATH, \"w\") as f:\n",
    "    yaml.dump(CONFIG, f, sort_keys=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'trainer': {'max_epochs': 12, 'min_epochs': 5, 'enable_progress_bar': True, 'devices': 1, 'deterministic': False, 'precision': '16'}, 'seed': 42, 'patience': 3, 'train_bs': 8, 'valid_bs': 8, 'workers': 8, 'wandb_project': 'hms_v6_2', 'output_dir': '../models-hmsv6-2/hmsv6-convnexts-3imgs-w020-kl1-lrf', 'fold_dir': '/home/maxc/workspace/kaggle-hms/folds', 'train_folds': [0], 'train_dataset_group': 'unique_label', 'val_dataset_group': 'eeg_id', 'raw_eeg_dir': '/home/maxc/workspace/kaggle-hms/data/v6/raw_eegs_sosbandclip_2500', 'eeg_spec_dir': '/home/maxc/workspace/kaggle-hms/data/v6/eeg_specs_h100w250gf5fft1024wl200lc05_norm_const', 'long_spec_dir': '/home/maxc/workspace/kaggle-hms/data/v6/long_specs', 'l7_weight': 0.2, 'inverse_kl_weight': True, 'inverse_kl_mean': [0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666, 0.16666666666666666], 'kl_multiplier': 1.0, 'vote_weight': False, 'img_types': ['long_spec', 'eeg_spec', 'full_raw_eeg', 'raw_eeg'], 'signals': ['LL', 'RL', 'LP', 'RP'], 'lrflip_signals': ['RL', 'LL', 'RP', 'LP'], 'img_size': [512, 512], 'long_spec_ratio': 1.0, 'raw_full_gap': 16, 'center_timespan': 10, 'sub_img_size': {'eeg_spec': [288, 200], 'long_spec': [288, 300], 'raw_eeg': [128, 500], 'full_raw_eeg': [64, 500]}, 'sub_img_vflips': {'eeg_spec': [False, False, False, False], 'long_spec': [False, False, False, False], 'raw_eeg': [False, False, True, True]}, 'train_aug_probs': {'lrflip_prob': 0.5, 'fbflip_prob': 0.5, 'mask_prob': 0.5, 'keep_center_ratio': 0.2, 'hflip_prob': 0.5, 'blur_prob': 0.0, 'roll_prob': 0.5, 'neg_eeg_prob': 0.5, 'contrast_prob': 0.5, 'fuse_prob': 0.0, 'block_prob': 0.5, 'noise_prob': 0.0, 'mask_iter': 3, 'mask_size_ratio': 0.2, 'num_block_ch': 4, 'dummy_votes_prob': 0.0, 'num_dummy_votes': 1}, 'val_aug_probs': {'lrflip_prob': 0.0, 'fbflip_prob': 0.0, 'mask_prob': 0.0, 'keep_center_ratio': 0.0, 'hflip_prob': 0.0, 'blur_prob': 0.0, 'roll_prob': 0.0, 'neg_eeg_prob': 0.0, 'contrast_prob': 0.0, 'fuse_prob': 0.0, 'block_prob': 0.0, 'noise_prob': 0.0, 'mask_iter': 5, 'mask_size_ratio': 0.1, 'num_block_ch': 4, 'dummy_votes_prob': 0.0, 'num_dummy_votes': 1}, 'model_type': 'SpecModel', 'dropout': 0.5, 'global_pool': ['avg'], 'hidden_size': 8, 'spec_backbone': 'convnext_small.fb_in22k', 'vit_model': 'vit_small_patch16_224', 'feature_layer': [-1], 'optimizer_params': {'lr': 0.0001}, 'use_ema': True, 'ema_decay': 0.999, 'scheduler': {'name': 'CosineAnnealingLR', 'params': {'CosineAnnealingLR': {'T_max': 10, 'eta_min': 1e-06}}}}\n",
      "Training fold 0 ======================\n",
      "{'lrflip_prob': 0.5, 'fbflip_prob': 0.5, 'mask_prob': 0.5, 'keep_center_ratio': 0.2, 'hflip_prob': 0.5, 'blur_prob': 0.0, 'roll_prob': 0.5, 'neg_eeg_prob': 0.5, 'contrast_prob': 0.5, 'fuse_prob': 0.0, 'block_prob': 0.5, 'noise_prob': 0.0, 'mask_iter': 3, 'mask_size_ratio': 0.2, 'num_block_ch': 4, 'dummy_votes_prob': 0.0, 'num_dummy_votes': 1}\n",
      "{'lrflip_prob': 0.0, 'fbflip_prob': 0.0, 'mask_prob': 0.0, 'keep_center_ratio': 0.0, 'hflip_prob': 0.0, 'blur_prob': 0.0, 'roll_prob': 0.0, 'neg_eeg_prob': 0.0, 'contrast_prob': 0.0, 'fuse_prob': 0.0, 'block_prob': 0.0, 'noise_prob': 0.0, 'mask_iter': 5, 'mask_size_ratio': 0.1, 'num_block_ch': 4, 'dummy_votes_prob': 0.0, 'num_dummy_votes': 1}\n",
      "2006 428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmaxc303\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.6 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>../models-hmsv6-2/hmsv6-convnexts-3imgs-w020-kl1-lrf/fold_0/wandb/run-20240411_085805-4exq6lph</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/maxc303/hms_v6_2/runs/4exq6lph/workspace' target=\"_blank\">hmsv6-convnexts-3imgs-w020-kl1-lrf_fold_0</a></strong> to <a href='https://wandb.ai/maxc303/hms_v6_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/maxc303/hms_v6_2' target=\"_blank\">https://wandb.ai/maxc303/hms_v6_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/maxc303/hms_v6_2/runs/4exq6lph/workspace' target=\"_blank\">https://wandb.ai/maxc303/hms_v6_2/runs/4exq6lph/workspace</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc/miniconda3/envs/hms/lib/python3.10/site-packages/lightning_fabric/connector.py:563: `precision=16` is supported for historical reasons but its usage is discouraged. Please set your precision to 16-mixed instead!\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using SpecModel\n",
      "Using EMA model with decay 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc/miniconda3/envs/hms/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:653: Checkpoint directory /home/maxc/workspace/kaggle-hms/models-hmsv6-2/hmsv6-convnexts-3imgs-w020-kl1-lrf exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type          | Params\n",
      "--------------------------------------------\n",
      "0 | model     | SpecModel     | 49.5 M\n",
      "1 | kl_loss   | KLDivLoss     | 0     \n",
      "2 | ema_model | AveragedModel | 49.5 M\n",
      "--------------------------------------------\n",
      "98.9 M    Trainable params\n",
      "0         Non-trainable params\n",
      "98.9 M    Total params\n",
      "395.662   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d0aa011c2b4311b4ea3a78880190da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc/miniconda3/envs/hms/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:77: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 8. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(106800, 7) (16, 7) (16, 7)\n",
      "(106800, 7) (6, 7) (6, 7)\n",
      "Score:  1.6747184724475055 G10 Score:  1.1537897069518372\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d816e3e448e846b794fe08ab3b785e0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maxc/miniconda3/envs/hms/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/maxc/miniconda3/envs/hms/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ed1cfb5bb44b7d9af9543530029c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.593 MB of 0.610 MB uploaded\\r'), FloatProgress(value=0.9724268124401271, max=1.0â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">hmsv6-convnexts-3imgs-w020-kl1-lrf_fold_0</strong> at: <a href='https://wandb.ai/maxc303/hms_v6_2/runs/4exq6lph/workspace' target=\"_blank\">https://wandb.ai/maxc303/hms_v6_2/runs/4exq6lph/workspace</a><br/>Synced 6 W&B file(s), 8 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>../models-hmsv6-2/hmsv6-convnexts-3imgs-w020-kl1-lrf/fold_0/wandb/run-20240411_085805-4exq6lph/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "config_path = CONFIG_PATH\n",
    "\n",
    "ckpt_path = None\n",
    "\n",
    "train_model(config_path, seed=CONFIG[\"seed\"] ,ckpt_path=ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unet-seg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
